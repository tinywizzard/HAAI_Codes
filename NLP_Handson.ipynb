{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6K+mqIKOdb1HM+kyBTP9B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinywizzard/HAAI_Codes/blob/main/NLP_Handson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Natural Language Processing (NLP) Introduction:** Natural Language Processing (NLP) is an interdisciplinary field merging computer science, linguistics, and machine learning. Its main objective is to allow computers to comprehend and handle human language naturally and effectively. NLP tasks are generally divided into two key categories:\n",
        "\n",
        "**(i) Natural Language Understanding (NLU):** This focuses on interpreting and understanding human language, including tasks like speech recognition, text classification, sentiment analysis, and extracting information from text.\n",
        "\n",
        "**(ii) Natural Language Generation (NLG):** This focuses on producing human-readable text from structured data, involving tasks like text summarization, dialogue generation, and language translation.\n",
        "\n",
        "**Examples:** NLP is applied in many fields, such as customer support (through chatbots and virtual assistants), content analysis (like sentiment analysis and topic modeling), and information retrieval (such as search engines and question-answering systems), among others.\n",
        "\n",
        "Important NLP Libraries in Python: Python provides a vast ecosystem of libraries and frameworks for performing NLP tasks. Below are some of the most popular and frequently used libraries:\n",
        "\n",
        "**(i) NLTK (Natural Language Toolkit):** NLTK is a highly versatile and widely used Python library for NLP tasks. It offers a comprehensive set of tools and resources for text processing, including functionalities like tokenization, stemming, lemmatization, part-of-speech tagging, and more, making it a go-to library for NLP practitioners.\n",
        "\n",
        "**(ii) spaCy:** spaCy is a high-performance library designed for advanced NLP tasks, offering powerful models for tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and more. It is well-regarded for its speed and ability to be easily integrated into production environments.\n",
        "\n",
        "**(iii) TextBlob:** TextBlob is a user-friendly library built on top of NLTK and Pattern, designed to streamline common NLP tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, and more. It simplifies these processes, making it easier for users to implement NLP functionality in their projects.\n",
        "\n",
        "**(iv) Gensim:** Gensim is a powerful library designed for topic modeling, offering optimized algorithms like Latent Dirichlet Allocation (LDA), Word2Vec, and Doc2Vec. It excels in tasks such as topic discovery, text similarity analysis, and generating word embeddings for better text representation.\n",
        "\n",
        "**1. Data Preprocessing as well as Text Cleaning**\n",
        "\n",
        "Before applying NLP techniques, it's crucial to preprocess and clean the text data to achieve accurate and dependable outcomes. Below are some commonly used preprocessing steps. italicized text\n",
        "\n",
        "Here is a Python example of basic data preprocessing and text cleaning using the **Natural Language Toolkit (NLTK)** and **regular expressions (re)**.\n",
        "\n",
        "Steps in the code:\n",
        "\n",
        "**(i)Lowercasing:** Converts the text to lowercase.\n",
        "\n",
        "**(ii)Removing Punctuation:** Strips out any punctuation or special characters.\n",
        "\n",
        "**(iii)Tokenization:** Splits the text into individual words.\n",
        "\n",
        "**(iv)Removing Stopwords:** Filters out common words like \"is\", \"and\", \"the\" that are not essential for NLP tasks.\n",
        "\n",
        "**(v)Lemmatization:** Reduces words to their base form (e.g., \"running\" → \"run\")."
      ],
      "metadata": {
        "id": "aPGBfM6sZBBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources (run only once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Example text\n",
        "text = \"This is an example sentence, to demonstrate text preprocessing! We'll clean it and tokenize.\"\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # 1. Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # 3. Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 4. Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # 5. Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Process the example text\n",
        "cleaned_text = preprocess_text(text)\n",
        "print(\"Cleaned Tokens:\", cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjFBGYArXn2g",
        "outputId": "bc576fbf-d27d-4ec1-e3b5-b7b4a1123813"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Tokens: ['example', 'sentence', 'demonstrate', 'text', 'preprocessing', 'well', 'clean', 'tokenize']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script covers basic preprocessing techniques, which are essential for preparing raw text data for more advanced NLP tasks.\n",
        "\n",
        "**2. Part-of-speech tagging and Named Entity Recognition**\n",
        "\n",
        "Here is a Python example using the **spaCy** library for **Part-of-Speech (POS) tagging** and **Named Entity Recognition (NER)**.\n",
        "\n",
        "Background concepts and steps in the code:\n",
        "\n",
        "**(i)Load spaCy’s model:** en_core_web_sm is a lightweight model that includes tokenization, POS tagging, and NER.\n",
        "\n",
        "**(ii)Part-of-Speech Tagging:** Each word in the text is tagged with its grammatical role, such as noun, verb, adjective, etc. Here’s a simple example:\n",
        "\n",
        "Sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "**PoS Tags:**\n",
        "\n",
        "The - Determiner (DT)\n",
        "\n",
        "quick - Adjective (JJ)\n",
        "\n",
        "brown - Adjective (JJ)\n",
        "\n",
        "fox - Noun (NN)\n",
        "\n",
        "jumps - Verb (VBZ)\n",
        "\n",
        "over - Preposition (IN)\n",
        "\n",
        "the - Determiner (DT)\n",
        "\n",
        "lazy - Adjective (JJ)\n",
        "\n",
        "dog - Noun (NN)\n",
        "\n",
        "Here's a brief overview of other PoS tags for each of these categories:\n",
        "\n",
        "**Pronoun:**\n",
        "\n",
        "PRP (Personal Pronoun): I, you, he, she, it, we, they\n",
        "\n",
        "PRP$ (Possessive Pronoun): my, your, his, her, its, our, their Number:\n",
        "\n",
        "There isn’t a specific PoS tag for \"number\" itself, but numbers are often tagged as:\n",
        "\n",
        "**CD (Cardinal Number):** one, two, three, 42\n",
        "\n",
        "**Adverb:**\n",
        "\n",
        "RB (Adverb): quickly, very, well\n",
        "\n",
        "RBR (Adverb, Comparative): better, faster\n",
        "\n",
        "RBS (Adverb, Superlative): best, fastest\n",
        "\n",
        "**Punctuation:**\n",
        "\n",
        ". (Period) or PUNC (General Punctuation)\n",
        "\n",
        ", (Comma)\n",
        "\n",
        "? (Question Mark)\n",
        "\n",
        "! (Exclamation Mark)\n",
        "\n",
        "**Sign:**\n",
        "\n",
        "SYM (Symbol): $, %, &, @\n",
        "\n",
        "**(iii)Named Entity Recognition:** The named entities in the text are identified and classified.\n",
        "\n",
        "Here is an example:\n",
        "\n",
        "Sentence: \"Barack Obama visited the Eiffel Tower in Paris last summer.\"\n",
        "\n",
        "**NER Tags:**\n",
        "\n",
        "Barack Obama - Person (PER)\n",
        "\n",
        "Eiffel Tower - Location (LOC)\n",
        "\n",
        "Paris - Location (LOC)\n",
        "\n",
        "In this example, NER helps identify and categorize specific names and locations in the text."
      ],
      "metadata": {
        "id": "DeQX3rNRZq8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion. Steve Jobs founded Apple in 1976.\"\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Part-of-Speech (POS) tagging\n",
        "print(\"Part-of-Speech Tagging:\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.pos_} ({token.tag_})\")\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-raj33wxYNZE",
        "outputId": "30a7e7f5-13e0-4729-e72d-7e352f40870a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part-of-Speech Tagging:\n",
            "Apple: PROPN (NNP)\n",
            "is: AUX (VBZ)\n",
            "looking: VERB (VBG)\n",
            "at: ADP (IN)\n",
            "buying: VERB (VBG)\n",
            "U.K.: PROPN (NNP)\n",
            "startup: NOUN (NN)\n",
            "for: ADP (IN)\n",
            "$: SYM ($)\n",
            "1: NUM (CD)\n",
            "billion: NUM (CD)\n",
            ".: PUNCT (.)\n",
            "Steve: PROPN (NNP)\n",
            "Jobs: PROPN (NNP)\n",
            "founded: VERB (VBD)\n",
            "Apple: PROPN (NNP)\n",
            "in: ADP (IN)\n",
            "1976: NUM (CD)\n",
            ".: PUNCT (.)\n",
            "\n",
            "Named Entities:\n",
            "Apple: ORG (Companies, agencies, institutions, etc.)\n",
            "U.K.: GPE (Countries, cities, states)\n",
            "$1 billion: MONEY (Monetary values, including unit)\n",
            "Steve Jobs: PERSON (People, including fictional)\n",
            "Apple: ORG (Companies, agencies, institutions, etc.)\n",
            "1976: DATE (Absolute or relative dates or periods)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example demonstrates how to use spaCy for two key NLP tasks, POS tagging and NER, which are essential for understanding the structure and meaning of text."
      ],
      "metadata": {
        "id": "7UIGH_THb7rW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Sentiment Analysis**\n",
        "\n",
        "Here is an example of **sentiment analysis** in **Python** using the **TextBlob** library."
      ],
      "metadata": {
        "id": "RLv16CMmb9Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Sample text\n",
        "text = \"I love this product! It's fantastic and works like a charm.\"\n",
        "\n",
        "# Create a TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Perform sentiment analysis\n",
        "sentiment = blob.sentiment\n",
        "\n",
        "# Output sentiment polarity and subjectivity\n",
        "print(f\"Polarity: {sentiment.polarity}\")    # Polarity: -1 (negative) to 1 (positive)\n",
        "print(f\"Subjectivity: {sentiment.subjectivity}\")    # Subjectivity: 0 (objectivity) to 1 (subjective)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DQhTUZ4bY8Z",
        "outputId": "45f60aae-2982-4ae0-9142-df1504d79a21"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity: 0.5125\n",
            "Subjectivity: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "**(i)Polarity:** Measures the sentiment's positivity or negativity (-1 for negative, 0 for neutral, 1 for positive).\n",
        "\n",
        "**(ii)Subjectivity:** Ranges from 0 (objective) to 1 (subjective), indicating whether the text is based on fact or personal opinion.\n",
        "\n",
        "In this case, the **polarity score** of 0.5125 suggests that the text is **moderately positive** but not overwhelmingly so. The language conveys a generally favorable tone, though it may not be excessively enthusiastic or optimistic.\n",
        "\n",
        "With a **subjectivity score** of 0.75, the text likely contains a **strong personal bias, subjective expressions, or opinions**, rather than being purely based on facts or objective analysis.\n",
        "\n",
        "This code shows that the sample text is highly positive with a significant degree of subjectivity. **TextBlob** is simple yet effective for basic sentiment analysis tasks."
      ],
      "metadata": {
        "id": "NE-fd5ypesP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Topic Modeling and Document Clustering**\n",
        "\n",
        "Here is a Python example for **Topic Modeling and Document Clustering** using the **Gensim library** and **Latent Dirichlet Allocation (LDA)**. **Topic modeling** is a method used to identify underlying themes or topics within a collection of documents, while **document clustering** focuses on grouping documents that share similar content or themes.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "**(i)Preprocessing:** Tokenization, lowercasing, and removal of stopwords.\n",
        "\n",
        "**(ii)Dictionary and Document-Term Matrix:** Created using Gensim for converting text data into numerical form.\n",
        "\n",
        "**(iii)LDA Model:** A Latent Dirichlet Allocation model is trained with 2 topics.\n",
        "\n",
        "**(iv)Document Clustering:** Document similarity is demonstrated using cosine similarity between two documents."
      ],
      "metadata": {
        "id": "CK2d0JdlfFYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords (run only once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"Artificial intelligence is transforming the technology industry.\",\n",
        "    \"Machine learning and AI are shaping the future of automation.\",\n",
        "    \"Deep learning algorithms are a subset of machine learning.\",\n",
        "    \"Quantum computing will revolutionize industries like AI.\",\n",
        "    \"Healthcare is benefiting from AI and machine learning advances.\",\n",
        "]\n",
        "\n",
        "# Preprocess the documents\n",
        "def preprocess(doc):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(doc.lower())\n",
        "    return [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "processed_docs = [preprocess(doc) for doc in documents]\n",
        "\n",
        "# Create a dictionary and document-term matrix\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "# Train the LDA model (specifying 2 topics)\n",
        "lda_model = LdaModel(doc_term_matrix, num_topics=2, id2word=dictionary, passes=15)\n",
        "\n",
        "# Print the topics with associated words\n",
        "print(\"Topics discovered by LDA:\")\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "\n",
        "# Document similarity (clustering example)\n",
        "doc1_bow = dictionary.doc2bow(preprocess(\"AI and machine learning are advancing rapidly\"))\n",
        "doc2_bow = dictionary.doc2bow(preprocess(\"Healthcare is benefiting from AI advances\"))\n",
        "\n",
        "similarity = gensim.matutils.cossim(doc1_bow, doc2_bow)\n",
        "print(\"\\nDocument Similarity (cosine):\", similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZl6qu3aeqnW",
        "outputId": "55ef1fda-4c02-4cfd-ab5b-230e5c77713d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics discovered by LDA:\n",
            "(0, '0.113*\"ai\" + 0.065*\"like\" + 0.065*\"quantum\" + 0.065*\"industries\" + 0.065*\"computing\"')\n",
            "(1, '0.129*\"learning\" + 0.091*\"machine\" + 0.053*\"algorithms\" + 0.053*\"deep\" + 0.053*\"subset\"')\n",
            "\n",
            "Document Similarity (cosine): 0.2886751345948129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "In this example, the **Gensim library** is used for both topic modeling and document clustering. We begin by preparing a list of sample documents, then create a dictionary and a document-term matrix from them. After that, a Latent Dirichlet Allocation (LDA) model is trained, specifying how many topics we aim to identify. The print_topics method is used to display the discovered topics along with their associated words. Additionally, document similarity is demonstrated by converting two documents into bag-of-words vectors and calculating their cosine similarity using gensim.matutils.cossim. These examples provide a basic introduction to NLP with Python, setting the stage for more advanced techniques such as text generation, machine translation, and question answering.\n",
        "\n",
        "The **coefficient values** (e.g., 0.070, 0.091, etc.) in the topics discovered by **LDA** represent the probability or weight of each word in the corresponding topic. These values indicate how relevant each word is to a given topic. For example, in Topic 0, \"artificial\", \"transforming\", \"industry\", \"intelligence\" and \"technology\" are equally important as they have equal weights i.e., 0.070. In Topic 1, \"learning\" has a weight of 0.127, which makes it even more central to this topic.\n",
        "\n",
        "A **cosine similarity** score of 0.289 (approximately) indicates a moderate to low level of similarity between two documents. This means the documents share some overlap in terms of words or content, but they are more different than similar.\n",
        "\n",
        "**Remember:** Cosine similarity is a measure of how similar two documents are, based on the angle between their vector representations in a multi-dimensional space. It ranges from -1 to 1, where:\n",
        "\n",
        "(i) 1 indicates that the documents are identical in terms of their direction (completely similar).\n",
        "\n",
        "(ii) 0 means the documents are completely different and share no similarities.\n",
        "\n",
        "(iii) -1 suggests that the documents are opposites in meaning."
      ],
      "metadata": {
        "id": "75KlM8odh96l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusions:**\n",
        "\n",
        "Natural Language Processing (NLP) using Python empowers machines to understand and interpret human language. With Python's vast array of NLP libraries, one can perform various tasks, including cleaning and tokenizing text, analyzing sentiment, modeling topics, and clustering documents. These tools offer immense capabilities for developing intelligent language-based applications."
      ],
      "metadata": {
        "id": "Z-0CmBc3iNVp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_iYTwXCbhsOa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}